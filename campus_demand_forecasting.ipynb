{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campus Demand Forecasting with DeepAR\n",
    "\n",
    "This notebook trains a DeepAR model to forecast occupancy rates for educational resources and generates 24-hour predictions for 5 random resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SageMaker setup\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 path to your parquet file\n",
    "s3_path = '<your-s3-URI>'\n",
    "\n",
    "# Read parquet file\n",
    "df = pd.read_parquet(s3_path)\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Convert string columns to numeric where needed\n",
    "df['current_occupancy'] = pd.to_numeric(df['current_occupancy'], errors='coerce')\n",
    "df['occupancy_rate'] = pd.to_numeric(df['occupancy_rate'], errors='coerce')\n",
    "df['total_capacity'] = pd.to_numeric(df['total_capacity'], errors='coerce')\n",
    "df['availability_hours'] = pd.to_numeric(df['availability_hours'], errors='coerce')\n",
    "\n",
    "# Sort by resource_id and timestamp\n",
    "df = df.sort_values(['resource_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Data types after conversion:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features (populate the null lag columns)\n",
    "def create_lag_features(group):\n",
    "    # Sort by timestamp\n",
    "    group = group.sort_values('timestamp')\n",
    "    \n",
    "    # Create lag features\n",
    "    group['occupancy_rate_lag_1h'] = group['occupancy_rate'].shift(1)\n",
    "    group['occupancy_rate_lag_24h'] = group['occupancy_rate'].shift(24)\n",
    "    group['occupancy_rate_lag_168h'] = group['occupancy_rate'].shift(168)  # 1 week\n",
    "    \n",
    "    # Create rolling average for last 24 hours\n",
    "    group['occupancy_rate_avg_24h'] = group['occupancy_rate'].rolling(window=24, min_periods=1).mean()\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply lag features per resource\n",
    "df = df.groupby('resource_id', group_keys=False).apply(create_lag_features)\n",
    "\n",
    "# Fill remaining NaN values in lag features with 0 (for initial periods)\n",
    "lag_cols = ['occupancy_rate_lag_1h', 'occupancy_rate_lag_24h', 'occupancy_rate_lag_168h', 'occupancy_rate_avg_24h']\n",
    "df[lag_cols] = df[lag_cols].fillna(0)\n",
    "\n",
    "print(f\"\\nLag features created. Sample:\")\n",
    "print(df[['resource_id', 'timestamp', 'occupancy_rate'] + lag_cols].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create label encoders for categorical columns\n",
    "categorical_cols = ['day_of_week', 'is_exam_period', 'is_peak_hour', 'resource_type', 'location']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"{col}: {le.classes_}\")\n",
    "\n",
    "print(f\"\\nCategorical encoding complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR requires time series data in JSON format\n",
    "# We'll create train and test splits\n",
    "\n",
    "prediction_length = 24  # Forecast next 24 hours\n",
    "context_length = 168    # Use last 7 days (168 hours) as context\n",
    "\n",
    "# Get unique resources\n",
    "resources = df['resource_id'].unique()\n",
    "print(f\"Total resources: {len(resources)}\")\n",
    "\n",
    "# Split data: use last 24 hours for testing\n",
    "train_data = []\n",
    "test_data = []\n",
    "resource_mapping = []  # Track resource_id for each entry\n",
    "\n",
    "for resource in resources:\n",
    "    resource_df = df[df['resource_id'] == resource].sort_values('timestamp')\n",
    "    \n",
    "    # Skip if not enough data\n",
    "    if len(resource_df) < context_length + prediction_length:\n",
    "        continue\n",
    "    \n",
    "    # Get time series values\n",
    "    target = resource_df['occupancy_rate'].values.tolist()\n",
    "    start_time = resource_df['timestamp'].min()\n",
    "    \n",
    "    # Dynamic features (categorical encoded features)\n",
    "    dynamic_feat = []\n",
    "    for col in ['day_of_week_encoded', 'is_exam_period_encoded', 'is_peak_hour_encoded']:\n",
    "        if col in resource_df.columns:\n",
    "            dynamic_feat.append(resource_df[col].values.tolist())\n",
    "    \n",
    "    # Static categorical features\n",
    "    cat_features = []\n",
    "    if 'resource_type_encoded' in resource_df.columns:\n",
    "        cat_features.append(int(resource_df['resource_type_encoded'].iloc[0]))\n",
    "    if 'location_encoded' in resource_df.columns:\n",
    "        cat_features.append(int(resource_df['location_encoded'].iloc[0]))\n",
    "    \n",
    "    # Train data (all but last 24 hours)\n",
    "    train_entry = {\n",
    "        \"start\": str(start_time),\n",
    "        \"target\": target[:-prediction_length],\n",
    "    }\n",
    "    if dynamic_feat:\n",
    "        train_entry[\"dynamic_feat\"] = [feat[:-prediction_length] for feat in dynamic_feat]\n",
    "    if cat_features:\n",
    "        train_entry[\"cat\"] = cat_features\n",
    "    \n",
    "    train_data.append(train_entry)\n",
    "    \n",
    "    # Test data (full series for evaluation)\n",
    "    test_entry = {\n",
    "        \"start\": str(start_time),\n",
    "        \"target\": target,\n",
    "    }\n",
    "    if dynamic_feat:\n",
    "        test_entry[\"dynamic_feat\"] = dynamic_feat\n",
    "    if cat_features:\n",
    "        test_entry[\"cat\"] = cat_features\n",
    "    \n",
    "    test_data.append(test_entry)\n",
    "    resource_mapping.append(resource)  # Store the actual resource_id\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to JSON Lines format\n",
    "def write_jsonlines(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for entry in data:\n",
    "            json.dump(entry, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "write_jsonlines(train_data, 'train.json')\n",
    "write_jsonlines(test_data, 'test.json')\n",
    "\n",
    "print(\"Data files created: train.json, test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_data_path = f\"s3://{bucket}/deepar-occupancy-forecast\"\n",
    "s3_train_path = f\"{s3_data_path}/train/train.json\"\n",
    "s3_test_path = f\"{s3_data_path}/test/test.json\"\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('deepar-occupancy-forecast/train/train.json').upload_file('train.json')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('deepar-occupancy-forecast/test/test.json').upload_file('test.json')\n",
    "\n",
    "print(f\"Train data uploaded to: {s3_train_path}\")\n",
    "print(f\"Test data uploaded to: {s3_test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train DeepAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DeepAR container image\n",
    "image_name = sagemaker.image_uris.retrieve('forecasting-deepar', region)\n",
    "\n",
    "print(f\"DeepAR image: {image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DeepAR estimator\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge',\n",
    "    output_path=f\"s3://{bucket}/deepar-occupancy-forecast/output\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "estimator.set_hyperparameters(\n",
    "    time_freq='H',  # Hourly frequency\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    mini_batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    "    num_cells=40,\n",
    "    num_layers=3,\n",
    "    likelihood='gaussian',\n",
    "    dropout_rate=0.1,\n",
    "    embedding_dimension=10,\n",
    "    num_dynamic_feat=len(dynamic_feat) if dynamic_feat else 0,\n",
    "    cardinality=json.dumps([len(le.classes_) for le in [label_encoders.get('resource_type'), label_encoders.get('location')] if le is not None])\n",
    ")\n",
    "\n",
    "print(\"Estimator configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "data_channels = {\n",
    "    \"train\": s3_train_path,\n",
    "    \"test\": s3_test_path\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy Model and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate 24-Hour Forecasts for 5 Random Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 random resources\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(test_data), size=min(5, len(test_data)), replace=False)\n",
    "random_test_samples = [(test_data[i], resource_mapping[i]) for i in random_indices]\n",
    "\n",
    "print(f\"Selected {len(random_test_samples)} random resources for prediction\")\n",
    "print(f\"Resource IDs: {[r[1] for r in random_test_samples]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = []\n",
    "\n",
    "for idx, (sample, resource_id) in enumerate(random_test_samples):\n",
    "    # Prepare input (use all but last 24 hours)\n",
    "    input_data = {\n",
    "        \"instances\": [{\n",
    "            \"start\": sample[\"start\"],\n",
    "            \"target\": sample[\"target\"][:-prediction_length]\n",
    "        }],\n",
    "        \"configuration\": {\n",
    "            \"num_samples\": 100,\n",
    "            \"output_types\": [\"mean\", \"quantiles\"],\n",
    "            \"quantiles\": [\"0.1\", \"0.5\", \"0.9\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add dynamic features if present - MUST include full prediction range\n",
    "    if \"dynamic_feat\" in sample:\n",
    "        # Include historical + future (full series)\n",
    "        input_data[\"instances\"][0][\"dynamic_feat\"] = sample[\"dynamic_feat\"]\n",
    "    \n",
    "    # Add categorical features if present\n",
    "    if \"cat\" in sample:\n",
    "        input_data[\"instances\"][0][\"cat\"] = sample[\"cat\"]\n",
    "    \n",
    "    # Get prediction\n",
    "    result = predictor.predict(input_data)\n",
    "    predictions.append({\n",
    "        \"resource_id\": resource_id,\n",
    "        \"actual\": sample[\"target\"][-prediction_length:],\n",
    "        \"predicted_mean\": result[\"predictions\"][0][\"mean\"],\n",
    "        \"predicted_quantiles\": result[\"predictions\"][0][\"quantiles\"]\n",
    "    })\n",
    "\n",
    "print(f\"Generated predictions for {len(predictions)} resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(len(predictions), 1, figsize=(15, 4*len(predictions)))\n",
    "\n",
    "if len(predictions) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    ax = axes[idx]\n",
    "    hours = list(range(1, prediction_length + 1))\n",
    "    \n",
    "    # Plot actual values\n",
    "    ax.plot(hours, pred[\"actual\"], label='Actual', marker='o', linewidth=2)\n",
    "    \n",
    "    # Plot predicted mean\n",
    "    ax.plot(hours, pred[\"predicted_mean\"], label='Predicted Mean', marker='s', linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    ax.fill_between(\n",
    "        hours,\n",
    "        pred[\"predicted_quantiles\"][\"0.1\"],\n",
    "        pred[\"predicted_quantiles\"][\"0.9\"],\n",
    "        alpha=0.3,\n",
    "        label='80% Confidence Interval'\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Hours Ahead')\n",
    "    ax.set_ylabel('Occupancy Rate')\n",
    "    ax.set_title(f'Resource {pred[\"resource_id\"]}: 24-Hour Occupancy Rate Forecast')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('occupancy_forecasts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'occupancy_forecasts.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate Prediction Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Calculate metrics for each resource\n",
    "metrics_summary = []\n",
    "\n",
    "for pred in predictions:\n",
    "    actual = np.array(pred[\"actual\"])\n",
    "    predicted = np.array(pred[\"predicted_mean\"])\n",
    "    \n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mape = np.mean(np.abs((actual - predicted) / (actual + 1e-10))) * 100\n",
    "    \n",
    "    metrics_summary.append({\n",
    "        'Resource': pred[\"resource_id\"],\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE (%)': mape\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(\"\\nPrediction Metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(metrics_df.mean().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the endpoint when done\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded occupancy data from S3 parquet file\n",
    "2. Created lag features (1h, 24h, 168h) and rolling averages\n",
    "3. Encoded categorical features for machine learning\n",
    "4. Prepared data in DeepAR JSON format\n",
    "5. Trained a DeepAR forecasting model\n",
    "6. Generated 24-hour forecasts for 5 random resources\n",
    "7. Visualized predictions with confidence intervals\n",
    "8. Calculated prediction accuracy metrics\n",
    "\n",
    "The model can now be used to forecast occupancy rates for any resource in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
